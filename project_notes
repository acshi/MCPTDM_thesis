Joint Multi-Policy Behavior Estimation and Receding-HorizonTrajectory Planning for Automated Urban Driving
This paper considers a T-intersection scenario. The paper models obstacle vehicles with several reference trajectories/policies, that each cover a different intent (go straight, turn, stop). It uses a POMDP solver to determine what the optimal speeds are to go through each trajectory, and the POMDP solver is run individually for each obstacle vehicle, only considering that it should avoid the ego vehicle, make progress, and prefer null actions for smoothness. It uses a (joint) particle filter to determine the probabilities that each obstacle vehicle is following each policy, but then reduces this to individual probabilities for each obstacle vehicle that serve as weights. At each future point in time an ellipse is computed for each obstacle vehicle that bounds the location is must be in with a 99.9% probability (for example). This is done by first computing ellipses at each point in time for each policy trajectory, using the speeds calculated by the POMDP solver, and applying some unspecified (as far as I can tell reading the paper, though it may be taken from the particle filter somehow) observation uncertaintly. (If any of these different ellipses overlap, they are combined into one bounding ellipse, otherwise they are held separate.... the paper seems ambiguous/contradictory on this part). The level-set is then computed from this sum-gaussian mixture, to 99.9% (or whatever threshold) bound the total area. Then the non-permitted ego positions are computed as the minkowski sum of the level-set and the shape of the ego vehicle, which they represent with four discs, to simplify the representation... though they don't say what that ends up looking like. These non-permitted ego positions are an input to the MPCC (model predictive contour control) planner, which uses them in a non-linear trajectory optimization for the ego vehicle which is not _exactly_ contrained to following any policy, but does still start with a reference trajectory to follow. MPCC minimizes an error function that takes into account following the reference trajectory (lag and contour components), making progress, and minimizing actions (for smoothness), while having hard contraints that it must avoid any non-permitted position. Naturally, the most computationally expensive part of this is the POMDP part, which actually isn't even all that helpful, and skipping it and assuming constant velocity does almost as well. Their evaluation uses four obstacle vehicles, obviously because they have a quad core laptop, to run each POMDP in parallel.

What do we want to do from this: Should we consider the T-intersection scenario? Should we design trajectories for the obstacle vehicles? Then we don't have to consider the obstacle vehicles maybe colliding with each other at all! Their trajectories can just be designed for prevent that! On the other hand, this paper has relatively weak evaluation, only considering exactly the one scenario they set up. My intent with randomizing "personality" of each obstacle vehicle was to generate essentially infinite possible scenarios. That just has the downside of them needing to be smart enough to not all collide with each other.

Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching (EUDM)
This paper considers both a double merge and also a "ring" scenario, and makes direct comparison with MPDM. Full code is at https://github.com/HKUST-Aerial-Robotics/eudm_planner, and quite a few details are left to the code. Vehicles are modelled with the "intellgent driving model" (longitudinal control) and the "pure pursuit controller" (lateral control), and with MPDM policies. Trajectories are generated with a "spatio-temporal corridor structure" at least for the ego vehicle, and potentially the obstacle vehicles too? Belief about the policies of obstacle vehicles are computed through simple heuristics (see the code). Reward functions are also relegated to looking up the code. There are six policies, three lateral and three longitudinal (lane change left, lane change, right, lane keep, accelerate, maintain speed, and decelerate). The accelerate and decelerate policies try to achieve a new set velocity (instead of continuously accelerate). It would appear that lane keeping and maintaining speed... are both essentially the same maintain-status-quo policy? The meat of EUDM is the Domain-specific Closed-loop Policy Tree (DCP-Tree) and Conditional Focused Branching (CFB). The DCP-Tree is a search tree over policies that is allowed to branch only once and must start with keeping the current policy. They use a tree of depth 4, with each policy action lasting for 2 seconds, for a total 8 second horizon. CFB is how they determine risky scenarios with the obstacle vehicles. First, nearby vehicles are chosen as key vehicles. If we have a strong prediction about their policy, then we are done with them. If we are uncertain (similar belief values for multiple top policies), we open-loop simulate each policy for that vehicle to see how bad it could be (perhaps all other vehicles just maintain speed and heading? it isn't specified in the paper). If it isn't bad, then we just say heck, and choose the most likely policy, even though it is uncertain. For any vehicles with bad outcomes, we then do closed-loop simulations (in which all the safe vehicles are just doing their fixed most-likely policy) for each ego vehicle policy, and evaluate what the final reward is for that ego-vehicle policy, which we then select.

What do we want to do from this: There are a lot of components they use (their vehicle models, trajectory generation), that may be useful to start from? This needs more investigation. I like the DCP-Tree idea, as a very simple MPDM extension, and would love to take that. CFB is a nice heuristic, that could maybe be modified? Or made more justified in some way?

Safe Trajectory Generation for Complex Urban Environments Using Spatio-Temporal Semantic Corridor


Misc. Notes:
Stanley controller is similar to Pure Pursuit, but has some advantages.

Tasks:
Overall for this week: finalize the driving scenario we are comparing against, and get something dumb executing it.
Read paper Safe Trajectory Generation paper...
Implement bicycle model.
Lane change maneuver based on simple pure-pursuit.

Finished tasks:
Intellgient driving model!
