Joint Multi-Policy Behavior Estimation and Receding-HorizonTrajectory Planning for Automated Urban Driving
This paper considers a T-intersection scenario. The paper models obstacle vehicles with several reference trajectories/policies, that each cover a different intent (go straight, turn, stop). It uses a POMDP solver to determine what the optimal speeds are to go through each trajectory, and the POMDP solver is run individually for each obstacle vehicle, only considering that it should avoid the ego vehicle, make progress, and prefer null actions for smoothness. It uses a (joint) particle filter to determine the probabilities that each obstacle vehicle is following each policy, but then reduces this to individual probabilities for each obstacle vehicle that serve as weights. At each future point in time an ellipse is computed for each obstacle vehicle that bounds the location is must be in with a 99.9% probability (for example). This is done by first computing ellipses at each point in time for each policy trajectory, using the speeds calculated by the POMDP solver, and applying some unspecified (as far as I can tell reading the paper, though it may be taken from the particle filter somehow) observation uncertaintly. (If any of these different ellipses overlap, they are combined into one bounding ellipse, otherwise they are held separate.... the paper seems ambiguous/contradictory on this part). The level-set is then computed from this sum-gaussian mixture, to 99.9% (or whatever threshold) bound the total area. Then the non-permitted ego positions are computed as the minkowski sum of the level-set and the shape of the ego vehicle, which they represent with four discs, to simplify the representation... though they don't say what that ends up looking like. These non-permitted ego positions are an input to the MPCC (model predictive contour control) planner, which uses them in a non-linear trajectory optimization for the ego vehicle which is not _exactly_ contrained to following any policy, but does still start with a reference trajectory to follow. MPCC minimizes an error function that takes into account following the reference trajectory (lag and contour components), making progress, and minimizing actions (for smoothness), while having hard contraints that it must avoid any non-permitted position. Naturally, the most computationally expensive part of this is the POMDP part, which actually isn't even all that helpful, and skipping it and assuming constant velocity does almost as well. Their evaluation uses four obstacle vehicles, obviously because they have a quad core laptop, to run each POMDP in parallel.

What do we want to do from this: Should we consider the T-intersection scenario? Should we design trajectories for the obstacle vehicles? Then we don't have to consider the obstacle vehicles maybe colliding with each other at all! Their trajectories can just be designed for prevent that! On the other hand, this paper has relatively weak evaluation, only considering exactly the one scenario they set up. My intent with randomizing "personality" of each obstacle vehicle was to generate essentially infinite possible scenarios. That just has the downside of them needing to be smart enough to not all collide with each other.

Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching (EUDM)
This paper considers both a double merge and also a "ring" scenario, and makes direct comparison with MPDM. Full code is at https://github.com/HKUST-Aerial-Robotics/eudm_planner, and quite a few details are left to the code. Vehicles are modelled with the "intellgent driving model" (longitudinal control) and the "pure pursuit controller" (lateral control), and with MPDM policies. Trajectories are generated with a "spatio-temporal corridor structure" at least for the ego vehicle, and potentially the obstacle vehicles too? Belief about the policies of obstacle vehicles are computed through simple heuristics (see the code). Reward functions are also relegated to looking up the code. There are six policies, three lateral and three longitudinal (lane change left, lane change, right, lane keep, accelerate, maintain speed, and decelerate). The accelerate and decelerate policies try to achieve a new set velocity (instead of continuously accelerate). It would appear that lane keeping and maintaining speed... are both essentially the same maintain-status-quo policy? The meat of EUDM is the Domain-specific Closed-loop Policy Tree (DCP-Tree) and Conditional Focused Branching (CFB). The DCP-Tree is a search tree over policies that is allowed to branch only once and must start with keeping the current policy. They use a tree of depth 4, with each policy action lasting for 2 seconds, for a total 8 second horizon. CFB is how they determine risky scenarios with the obstacle vehicles. First, nearby vehicles are chosen as key vehicles. If we have a strong prediction about their policy, then we are done with them. If we are uncertain (similar belief values for multiple top policies), we open-loop simulate each policy for that vehicle to see how bad it could be (perhaps all other vehicles just maintain speed and heading? it isn't specified in the paper). If it isn't bad, then we just say heck, and choose the most likely policy, even though it is uncertain. For any vehicles with bad outcomes, we then do closed-loop simulations (in which all the safe vehicles are just doing their fixed most-likely policy) for each ego vehicle policy, and evaluate what the final reward is for that ego-vehicle policy, which we then select.

What do we want to do from this: There are a lot of components they use (their vehicle models, trajectory generation), that may be useful to start from? This needs more investigation. I like the DCP-Tree idea, as a very simple MPDM extension, and would love to take that. CFB is a nice heuristic, that could maybe be modified? Or made more justified in some way?

Safe Trajectory Generation for Complex Urban Environments Using Spatio-Temporal Semantic Corridor
...

Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments
This paper considers an urban driving situation with an all-way stop intersection, and 0-5 other vehicles. The car has to get to the intersection, wait for its turn to cross, cross safely, and then get back up to speed. There may be a stopped vehicle on the road that it has to get around. The obstacle vehicles follow a simple and agressive policy: they maintain a 6-meter follow distance, they accelerate at 1m/s^2 and brake at 2m/s^2, they will stop at the intersection and wait their turn, they otherwise try to maintain the 25mph speed limit. They don't change lanes. The ego vehicle is controlled by a "multi-level policy", with both low-level control policies and high-level "option" policies. Both of these are learned through reinforcement learning, by simulating examples for the deep learning (not that deep, only one hidden layer) to learn from. The cost function is complicated and not completely specified in the paper, and includes the current state, bonuses for completing goals, and penalities for violating constraints. It penalizes acceleration and steering angle rate and their derivatives (like jerk), and lateral acceleration as well. It includes penalties for being off from the reference speed (which I suppose changes when the vehicle is supposed to stop at the intersection) as well as offset from the center of the road. They use an l2 norm of weighted costs. The paper uses "Linear Temporal Logic" (LTL) to specify the rules of successfully completing goals. Violating these LTL rules or having a collision both result in a terminal condition with a penalty. Learning is done with a vector of derived features for the ego vehicle and a smaller set for all the nearby/relevant vehicles. A variety of boolean predicates from the LTL are also included as features. They use MCTS to actually choose the next option policy, but augment it by using the expected value of policies from the deep q-learning network to guide the search, instead of the more typical UCB-style. They also use "Progressive Widening" which limits the maximum number of children of a given node, and apparently helps with MCTS in large spaces.

Monte Carlo Tree Search in Continuous Action Spaces with Execution Uncertainty
This paper considers the game of Curling, which has similaries to Bocci Ball, but is played in the Winter Olympics. It uses MCTS in a continuous action space, and proposes Kernel Regression Upper Confidence Bound for Trees (KR-UCT), a modification of the classic MCTS with UCB (i.e. UCT) algorithm. The principal idea is that like in a discrete space, we start off with a set of possible actions. When we choose which action to take in refining the tree, we use kernel regression to estimate the value of each action. So, a kernel function is selected that provides a similarity metric between two actions, and the whole set of actions is used to evaluate the value of each, using that kernel-weighted mean. Progressive Widening is also employed in a modified way. Normal progressive widening (also called unpruning) starts with a limited set of actions that can be taken, but over time as more samples are taken of a given node, new actions are progressively added to that node. In KR-UCT, new actions are added when specified by progressive widening, but because of the continuous action space, they are chosen to be (approximately) the action with the highest kernel-estimated value and the lowest kernel density -- essentially meaning that if all continuous actions where included, this is the action that would be chosen by the UCT-like selection algorithm. Finally, for selecting the final action to take once searching is complete, the lower confidence bound (LCB) is used, essentially choosing the action that we are most confident will not do poorly, (a sort of opposite of the UCB idea).

Monte-Carlo Planning in Large POMDPs (POMCP)
The idea of POMCP is to use MCTS to solve a black-box POMDP. It builds the search tree by branching on _both_ actions and observations, and so the partially-observableness of the problem ends up taking effect by yielding different observations. It maintains the belief state as an unweighted particle filter. It must be unweighted because it has no way of knowing what the probabilities associated with anything are, exactly, since the POMDP is black-box. However, with enough particles you get the right distribution anyways. Simulations are performed following UCT, just with the slightly different tree structure branching on actions and observations. After choosing the best action, we get a corresponding true observation from the environment. Now our new belief state is derived from all the simulations we performed that ended up agreeing with both the action we took and the observation that occurred, one particle per consistent simulation. Particle reinvigoration (presumably by adding artificial noise) is also used to make sure we maintain diversity and enough particles. They inserted domain knowledge through the use of "preferred actions." When present, preferred actions where inserted into the tree with an initial value equal to the highest empirical value from a set of test runs, and with N=10 already, and furthermore, roll-outs beyond the end of the search tree proper where limited to preferred actions only. Without the preferred actions, those roll-outs would have been chosen uniformly. Importantly, after taking an action, POMCP does not start from an empty tree and the current state, but continues from the pruned version of the tree that is consistent with the action it took and the observation it received.

Advice from Ed:
Each of May's policies includes both lateral and longitudinal control, so that they are coupled. We want to avoid a situation in which we choose a trajectory and only thereafter choose velocities for it, which might be an impossible task. We want to let MPDM do most of the heavy work in chooseing velocity. So maybe we bundle our laternal and longitudal policies into one combined policy for MPDM to choose. Perhaps we always use the intelligen driver model, but we allow the policy to choose the parameters for it: desired follow time and desired velocity.... maybe just desired follow time? (if efficiency means how close we are to the target velocity, then changing that by itself might not do anything if we are in traffic.)
May's policies all have the ability to come to a sudden stop, but no reason for us to include that in our policies: keep them simpler. He is happy for my lane change policies to just go for it, requiring MPDM to tell them not to if it would result in a crash.
His pure pursuit is simpler than mine, and avoids the circle intersection, doing some sort of curve integral to get the target point. Just not sure how they decide where to start counting!
There are tons of potential reward functions... so Ed agrees I should just go with something, and try not to waste time messing with it much.

Add as few knobs and whistles as possible, since complexity can grow fast.

Misc. Notes:
Stanley controller is similar to Pure Pursuit, but has some advantages.
https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf
https://dingyan89.medium.com/three-methods-of-vehicle-lateral-control-pure-pursuit-stanley-and-mpc-db8cc1d32081
https://thomasfermi.github.io/Algorithms-for-Automated-Driving/Control/PurePursuit.html

Questions for Ed (PRIOR):
It seems like my random obstacle car policy-changes are causing the ego vehicle to sometimes be in an impossible situation. Another vehicle can crash into it without it having a chance to escape!!! Partially this might be because I allow the other vehicle to maybe have better acceleration. What parameters should I randomize for the obstacle vehicles and how? Should the ego always be top-performing?
(Probably doesn't matter too much)

My policies carry some state. Some of which gets reset when you switch to that policy. So that means you could switch to the policy... that you are already on, and this would give different behavior. Either we embrace this, and treat it as a different policy choice, or we maybe find some way to avoid having that state? How does this work for lane changes or for "maintaining" speed? (maintaining _which_ speed!!)
(cargo run --release rng_seed 2 :: method mpdm :: max_steps 2405 2406)
(Ed says, yeah, try to think of some way that we can get rid of state and still get similar-ish behavior? Maybe you have a maintain policy that gets a little slower over time, and so you add another maintain policy that does a little bit of acceleration to compensate! Then you get both ways at least. On the other hand, it can be nice to just allow emergent behavior and not fight against it. Sure maybe allow a policy reset choice if it seems like this behavior is absolutely important! But check first. Emergent behavior shouldn't scare you! Again, any way to rephrase the policy? Maybe we try to make a certain amount of dy/dx based on speed, so that we store our progress in the physical world! Store state in the physical world as much as possible.)

Should I try at all for a ring or double ring environment? Or keep it perfectly straight? A benefit to the ring is needing to slow down for the curves, and needing fewer vehicles to simulate.
(Would be nice if not too much work, but maybe not too important)

Since I don't have belief estimation, how about just a static set of beliefs based on similarity of the current policy to a new one?
(Sounds good! Just be able to use ground-truth for debugging.)

(MCTS can win because you can throw more policies at it!!!)

Questions for Ed:
Tell the story in terms of how what we really want to do is similar to tree search with infinite samples, but we can't because that is infeasible. We can talk about how MPDM tries to solve this, then talk about how EUDM expands the search space, and then say that we think even more flexibility is justified and that we do this with MCTS. Talk about how their CFB is a useful heuristic for improving our ability to sample the space and then how we modify it for MCTS.

7/16/21:
Terminology to use? Monte Carlo roll-outs samples particles, vs the number of trials I run to evaluate results.

It is okay to elect policies at a rate slower than 20Hz, la!! There is enough to gain from using more samples, that we should be okay to do that... although we seem to have something going on with more samples not helping the final scenario? We _should_ use more samples though for the synthetic experiments.

Work the word "robust" into the title, since that is the main thrust of the changes we are doing.

Some decisions:
Increasing search depth appears to NOT actually help. Let's keep it at depth 4 and 8 seconds total.
Decided to have no placed obstacles, since cars can get a break policy and become obstacles on their own well enough!
Run simulations for a total of 60 seconds, since that feels clean and easy to understand.
As long as the smoothness weight isn't too high... it doesn't make much a difference! let's try setting to 0
Safety weight of 100 and above seem to all be about as good, but efficiency goes down for the higher ones, and safety isn't any better.
Uncomfortable deceleration weight of 5 seems about ideal, or close enough already anyway
Likewise the large curvature change weight of 2 seems reasonable, with little sensitivity.

hyper-parameter optimization for each method, at least of the discount factor, to optimize the evaluatuation results for that method, since what gives the best result for the evaluation may not be the same as for the cost function.

Ed's recommended order: contributions, abstract, results/evaluation, everything else, then related work maybe last (first pass, just dump citations, and get a sense for the reference look and organize). There is _some_ space in a journal article to editorialize a little about the field and the directions and the methods that you have.

EUDM uses 0 steer weight and plan_change weight of 2 or so

FROM ED:
give him the paper to do a "high level" review by end of Monday

Tasks:
Overall for this week:

OKAY, finalize discount factors for each method? (using few samples_n)

15 work days left until the end of July. A 20-page paper with an average of 1000 words per page, would be 20,000 words and I would need to write 1,333 words per day!!!

Make it use the real back-wheel pure pursuit...

Maybe do all these with n_samples=32:
UCB constant for MCTS?
Some comparison between KLUCB/+ and UCB?
Between marginal and lower_bound?

Then finally test using more samples?

Once everything else looks good with samples_n, we can try seeing what we get for additional policies, etc...

MAYBE SKIP:
A new idea: unequal layer times??? Or just try making search depth deeper?
A new idea: improve MCTS with progressive widening (I guess just adding new leaf nodes with different belief samples?) Or even a KR-UCT like approach that take advantage of the fact that policies with different longitudinal choices are more similar than ones with different lateral choices? Or even extend to continuous space with KR-UCT? Or use KR-UCT to make POMCP work?
A new idea: a machine-learning approach that chooses which paths of the tree to search?
POMCP proper? (maybe cut it)
Make an environment more like in the EUDM paper, like a loop, which will help reduce the number of cars on the road to a reasonable number for fast comparisons. They use 0.4 second time steps the closed loop simulations. (Their github uses 0.2 seconds timesteps, tree depth 5 and 1-second actions).

Finished tasks:
Figure out why EUDM is doing so poorly. Basically the same as "fixed" here.... :/
Synthetic test using z-score instead of prioritize_worst_particles_n... So it seems that "repeat everything" is coming up as the best option here... which intuitively seems really bad to me, showing that the synthetic test isn't being realistic at all. I imagine that trying to repeat all the particles across each top-level policy would just give us less diversity and make it less likely that we discover a really bad scenario which then itself might be repeated over and help decriminate between the different choices... but in the current setup, these "bad scenario" particles don't actually provide any information, since they are the same for all the choices. Maybe each (state, action) pair in the problem scenario should get not just a basic distribution, but also a "bad scenario" distribution. The true expected value then would be a combination of these and might be dominated by either one. Then, discovering enough "bad scenario" particles would be important to fish out the true value of these choices. Blindly repeating the basic particles would then not discover as much information.
So looking at the UCB constant... I wonder what is mattering more here... using the expected-cost rule with UCB to decide where the next node goes........ or just in determining which final action to choose! Because too many of them just seem to want (or tolerate) the highest UCB constant possible... which is suspicious!?
NEW IDEA: Motivation: we sometimes see a lot of changes between policies back and forth back and forth, because of how we have drawn the particles that each policy gets. Certain important particles end up having a big influence because they end up only affecting one policy and not another. SO, what if we "periodically" look at whichever particle is most discrediting a certain root-level policy choice and test how the best policy would react to this particle. Actually, we can just let our normal UCB/KLUCB choose the policy, and then we test this worst particle against it. And perhaps we can just make this _always_ be the rule, only falling back to normal particle selection if that worst particle has already been tested against the current policy. Hopefully, because of the min-avg way that the particles produce costs, this will have enough of an effect, although I could also imagine that there might be a collection of unluckily-bad particles conspiring together to make a particular policy look bad, and this wouldn't completely account for making that situation fair. Now, to test this in isolation: We can start from the same "progessive_mcts" setup, but now we need to have particles represented. Maybe we just have good (normal) and then also bad (always get high cost) particles. We can keep track of particles... with a BTreeSet? This is currently only really a problem when there is no _true_ _bad_ decision, when one is just marginally better than another, and then a bad particle makes one randomly _seem_ bad. So to simulate _that_ scenario... we need the bad particles to be a lot worse!!
Reform our cost function to not be discrete anywhere. use a sigmoid for safety. penalize all accelerations. the EUDM evaluation metrics can be kept on the side for reference, but not used in the cost function itself.
Random game trees to try to justify the "cost of node = max( average of downward partcicles, min(children))" and then also of reducing variance by subtracting off the mean of previous nodes in the search and keeping track of the marginal cost for each action instead. See if this can make us more sample-efficient! Do a bit of literature search to try to see if someone else has done this before.
We are getting into trouble because the obstacle vehicle policy waiting_done makes it not stateless, and so we are systematically mispredicting other vehicles. We want obstacle vehicles to not be extremely reckless, so they check their immediate area before _starting_ to turn. We want the ego vehicle to not be constantly scared because it has an obstacle-car model that says they may lane-change at any time. The POMDP of this would consider there to essentially be multiple policies here then, one that has checked and started turning, and one that hasn't.
With these changes, we expect to see MCTS not do worse when we use more samples. Also use fewer samples to check for trends!!
Alternative MCTS formulation where it samples from CFB, and so assigns equal weights to each sample.
Have MCTS sample from the CFB-selected scenarios different policies for all the unimportant vehicles.
Have MCTS use propagate up max(cost * probability weight) instead of avg(cost).
Is maybe EUDM going slower because the target_vel is just slower most of the time? It has very low optimization cost. Nope!!
Use different RNG for MCTS versus obstacle vehicle choices, so that the same rng seed produces a similar scenario regardless of method. This shouldn't necessarily change the asymptotic goodness of each method as we get more samples, but should make it converge faster with fewer samples.
Truly fair comparison needs to account for computation time. We can sweep samples_n.
Improve plotting so compare time and each metric, for each method. So for plotting, this means making it possible to draw a separate graph for each method.
cache aabb values
recycle obstacle vehicles between front and back (faster performance, better results)
Belief that uses a factor to decrease probability for policy change. (should fix CFB issues)
EUDM-style cost for velocity
Lane-change policy with storing progress in our remaining dy to finish.
Implement CFB
Report same overall reward metrics as the EUDM paper, _separate_ from the optimization cost function.. For example, no discount is applied to these overall rewards.
We need it to be possible to stop behind a vehicle and then still turn when it is safe to do so. Maybe we just need the intelligent driver model to only look at the target lane?
We need obstacle vehicles to be a _little_ nicer about when they change lanes, or we _will_ end up getting hit by vehicles willy-nilly! (We can make them perform the lane-change more slowly and also check there is nothing too close in the lane we want to change into)
Read that related paper from Max.
Proper accleration/deceleration policies, right now, we stay in an acceleration policy, and then that ineherently doesn't keep accelerating over time. Right now eudm does fine by being smart enough to switch in and out of acceleration policies, to ratchet up its speed. OOps!!!
Have a preliminary quantitative comparison between classic MPDM, "naive" EUDM, and a POMCP-inspired search.
Only tree is using the discount!!! Add for eudm and mpdm and mcts.
MCTS
Perform multiple trials, given we will have differently sampled states.
Sample belief state to intialize obstacle agent policies, instead of letting them be known correct!
Start maintaining a belief about what the other agents' policies might be. (Currently, the ego agent magically knows the policies, but not other info like desired velocity, etc...)
Make an evaluation setup and graph comparing the methods I have now and their rewards! Something super simple!
Obstacle vehicles need to be able to change policies/lanes. The easiest way to incorporate something like this is for the obstacle vehicles to be unable to crash with anything but the ego vehicle.
We should be able to display traces for the entire tree search! Very useful for debugging and understanding. Then do that for the eudm and mpdm styles, too.
EUDM-style tree search
ADD barrier to force merge!!!
Get basic MPDM running: lane change in any valid direction or lane keep.
Implement basic reward function and get something dumb executing it.
Intellgient driving model!
Implement bicycle model.
Lane change maneuver based on simple pure-pursuit.
Read paper Safe Trajectory Generation paper... (mostly anyway)
Finalize the driving scenario we are comparing against: track with a merge scenario, with a reward function: linear-weighted sum of efficiency (based on velocity and perhaps target velocity), safety (minimum distance to another vehicle), and consistency (staying on the same or a similar policy).

Much later tasks, NOT IMPORTANT now:
Modify pure pursuit to actually work as it is normally considered around the back axle. See if changing the bicycle model to be based on the back axle makes this work.
See if we can copy EUDM reward function, including constant weights and even how much of the track to evaluate before terminating.
We can make car-car closest-points WAY faster by writing specific code for that.
Get our testing setup and parameters to better match that used by the EUDM code: https://github.com/HKUST-Aerial-Robotics/eudm_planner/blob/master/eudm_planner/config/eudm_config.pb.txt
Maybe switch cars to a structure of arrays kind of thing?

Timeline:
6/25/21 -- Rough paper outline 20%
7/2/21 -- Form of final experiments determined through experimentation and full-system debugging
7/9/21 -- Paper at 60%
7/16/21 -- Paper at 80%, Ed reviews
7/23/21 -- Paper at 100%, reviewing and editing
7/30/21 -- (approx) due for submission


ASME Journal of Autonomous Vehicles and Systems
https://journaltool.asme.org/home/JournalDescriptions.cfm?JournalID=37&Journal=JAVS

Modeling and Simulation for Autonomous Ground Systems

We would appreciate receiving your contribution by the end of July 2021.Some flexibility in this date is possible, although it is constrained by the preliminary schedule to publish the special issue by the end of this year. Please let us know by April 25 if you plan to submit a manuscript.

Samples papers:
21 pages
Introduction
Background
Motivation
Analytical Consideration of Active Suspension
    Half-Car Vehicle Model
    Active Suspension Control System Design
    Suspension Control Settings and Related Analyses
Shaker Rig Test Results
    Shaker Rig
    Road Profile and Suspension Emulation
    Test Results
Active Seat Suspension Study and Design
    Analytical Study
    Active Seat Suspension Design
Conclusion
Acknowledgment
Conflict of Interest
Data Availability Statement
Appendix

9 pages
Introduction
    Motivation
    Project Overview and Goals
    Related Works and Paper Outline
Mechanism Overview
Manufacturing and Assembly
Digital Twin
Digital Twin Simulation
    Simulation Results Summary

10 pages
Introduction
    Related work
    Contributions and paper organization
Problem Formulation
Point-Mass Robot Problem Analysis
Representative Examples
Extension to Disc-Robot
Conclusion
Conflict of Interest
Appendix
