Joint Multi-Policy Behavior Estimation and Receding-HorizonTrajectory Planning for Automated Urban Driving
This paper considers a T-intersection scenario. The paper models obstacle vehicles with several reference trajectories/policies, that each cover a different intent (go straight, turn, stop). It uses a POMDP solver to determine what the optimal speeds are to go through each trajectory, and the POMDP solver is run individually for each obstacle vehicle, only considering that it should avoid the ego vehicle, make progress, and prefer null actions for smoothness. It uses a (joint) particle filter to determine the probabilities that each obstacle vehicle is following each policy, but then reduces this to individual probabilities for each obstacle vehicle that serve as weights. At each future point in time an ellipse is computed for each obstacle vehicle that bounds the location is must be in with a 99.9% probability (for example). This is done by first computing ellipses at each point in time for each policy trajectory, using the speeds calculated by the POMDP solver, and applying some unspecified (as far as I can tell reading the paper, though it may be taken from the particle filter somehow) observation uncertaintly. (If any of these different ellipses overlap, they are combined into one bounding ellipse, otherwise they are held separate.... the paper seems ambiguous/contradictory on this part). The level-set is then computed from this sum-gaussian mixture, to 99.9% (or whatever threshold) bound the total area. Then the non-permitted ego positions are computed as the minkowski sum of the level-set and the shape of the ego vehicle, which they represent with four discs, to simplify the representation... though they don't say what that ends up looking like. These non-permitted ego positions are an input to the MPCC (model predictive contour control) planner, which uses them in a non-linear trajectory optimization for the ego vehicle which is not _exactly_ contrained to following any policy, but does still start with a reference trajectory to follow. MPCC minimizes an error function that takes into account following the reference trajectory (lag and contour components), making progress, and minimizing actions (for smoothness), while having hard contraints that it must avoid any non-permitted position. Naturally, the most computationally expensive part of this is the POMDP part, which actually isn't even all that helpful, and skipping it and assuming constant velocity does almost as well. Their evaluation uses four obstacle vehicles, obviously because they have a quad core laptop, to run each POMDP in parallel.

What do we want to do from this: Should we consider the T-intersection scenario? Should we design trajectories for the obstacle vehicles? Then we don't have to consider the obstacle vehicles maybe colliding with each other at all! Their trajectories can just be designed for prevent that! On the other hand, this paper has relatively weak evaluation, only considering exactly the one scenario they set up. My intent with randomizing "personality" of each obstacle vehicle was to generate essentially infinite possible scenarios. That just has the downside of them needing to be smart enough to not all collide with each other.

Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching (EUDM)
This paper considers both a double merge and also a "ring" scenario, and makes direct comparison with MPDM. Full code is at https://github.com/HKUST-Aerial-Robotics/eudm_planner, and quite a few details are left to the code. Vehicles are modelled with the "intellgent driving model" (longitudinal control) and the "pure pursuit controller" (lateral control), and with MPDM policies. Trajectories are generated with a "spatio-temporal corridor structure" at least for the ego vehicle, and potentially the obstacle vehicles too? Belief about the policies of obstacle vehicles are computed through simple heuristics (see the code). Reward functions are also relegated to looking up the code. There are six policies, three lateral and three longitudinal (lane change left, lane change, right, lane keep, accelerate, maintain speed, and decelerate). The accelerate and decelerate policies try to achieve a new set velocity (instead of continuously accelerate). It would appear that lane keeping and maintaining speed... are both essentially the same maintain-status-quo policy? The meat of EUDM is the Domain-specific Closed-loop Policy Tree (DCP-Tree) and Conditional Focused Branching (CFB). The DCP-Tree is a search tree over policies that is allowed to branch only once and must start with keeping the current policy. They use a tree of depth 4, with each policy action lasting for 2 seconds, for a total 8 second horizon. CFB is how they determine risky scenarios with the obstacle vehicles. First, nearby vehicles are chosen as key vehicles. If we have a strong prediction about their policy, then we are done with them. If we are uncertain (similar belief values for multiple top policies), we open-loop simulate each policy for that vehicle to see how bad it could be (perhaps all other vehicles just maintain speed and heading? it isn't specified in the paper). If it isn't bad, then we just say heck, and choose the most likely policy, even though it is uncertain. For any vehicles with bad outcomes, we then do closed-loop simulations (in which all the safe vehicles are just doing their fixed most-likely policy) for each ego vehicle policy, and evaluate what the final reward is for that ego-vehicle policy, which we then select.

What do we want to do from this: There are a lot of components they use (their vehicle models, trajectory generation), that may be useful to start from? This needs more investigation. I like the DCP-Tree idea, as a very simple MPDM extension, and would love to take that. CFB is a nice heuristic, that could maybe be modified? Or made more justified in some way?

Safe Trajectory Generation for Complex Urban Environments Using Spatio-Temporal Semantic Corridor

Advice from Ed:
Each of May's policies includes both lateral and longitudinal control, so that they are coupled. We want to avoid a situation in which we choose a trajectory and only thereafter choose velocities for it, which might be an impossible task. We want to let MPDM do most of the heavy work in chooseing velocity. So maybe we bundle our laternal and longitudal policies into one combined policy for MPDM to choose. Perhaps we always use the intelligen driver model, but we allow the policy to choose the parameters for it: desired follow time and desired velocity.... maybe just desired follow time? (if efficiency means how close we are to the target velocity, then changing that by itself might not do anything if we are in traffic.)
May's policies all have the ability to come to a sudden stop, but no reason for us to include that in our policies: keep them simpler. He is happy for my lane change policies to just go for it, requiring MPDM to tell them not to if it would result in a crash.
His pure pursuit is simpler than mine, and avoids the circle intersection, doing some sort of curve integral to get the target point. Just not sure how they decide where to start counting!
There are tons of potential reward functions... so Ed agrees I should just go with something, and try not to waste time messing with it much.

Misc. Notes:
Stanley controller is similar to Pure Pursuit, but has some advantages.
https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf
https://dingyan89.medium.com/three-methods-of-vehicle-lateral-control-pure-pursuit-stanley-and-mpc-db8cc1d32081
https://thomasfermi.github.io/Algorithms-for-Automated-Driving/Control/PurePursuit.html

Tasks:
Overall for this week: Have a preliminary quantitative comparison between classic MPDM, "naive" EUDM, and a POMCP-inspired search.

We should be able to display traces for the entire tree search! Very useful for debugging and understanding. Then do that for the eudm and mpdm styles, too.

Right now, if we are part-way through executing the current action, and then decide to switch to a new action, we will reset that waiting period. Actually I think we reset it for the simulation even when we don't switch policies!!! I think EUDM proper keeps this all synchronized? NO! it doesn't! I don't think???

Reduce the number of cars on the road to a reasonable number for fast comparisons. They use 0.4 second time steps the closed loop simulations. (Online they use 0.2 seconds timesteps, tree depth 5 and 1-second actions).

Implement open-loop simulation of agents for either EUDM CFB, or for vehicles far enough away from the ego vehicle.


Finished tasks:
EUDM-style tree search
ADD barrier to force merge!!!
Get basic MPDM running: lane change in any valid direction or lane keep.
Implement basic reward function and get something dumb executing it.
Intellgient driving model!
Implement bicycle model.
Lane change maneuver based on simple pure-pursuit.
Read paper Safe Trajectory Generation paper... (mostly anyway)
Finalize the driving scenario we are comparing against: track with a merge scenario, with a reward function: linear-weighted sum of efficiency (based on velocity and perhaps target velocity), safety (minimum distance to another vehicle), and consistency (staying on the same or a similar policy).

Much later tasks, NOT IMPORTANT now:
Modify pure pursuit to actually work as it is normally considered around the back axle. See if changing the bicycle model to be based on the back axle makes this work.
See if we can copy EUDM reward function, including constant weights and even how much of the track to evaluate before terminating.
We can make car-car closest-points WAY faster by writing specific code for that.
Get our testing setup and parameters to better match that used by the EUDM code: https://github.com/HKUST-Aerial-Robotics/eudm_planner/blob/master/eudm_planner/config/eudm_config.pb.txt
Maybe switch cars to a structure of arrays kind of thing?
